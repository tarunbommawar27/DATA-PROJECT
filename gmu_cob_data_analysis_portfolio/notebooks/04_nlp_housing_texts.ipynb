{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1ed67f",
   "metadata": {},
   "source": [
    "# 04 â€” NLP on Housing Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "texts = pd.read_csv('data/policy_texts.csv')\n",
    "texts['label'] = texts['text'].str.contains(r'eviction|shelter|voucher|rehousing|legal', case=False, regex=True).astype(int)\n",
    "\n",
    "def clean(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r'[^a-z0-9\\s]', ' ', t)\n",
    "    return re.sub(r'\\s+', ' ', t).strip()\n",
    "\n",
    "texts['clean'] = texts['text'].map(clean)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=1, ngram_range=(1,2))\n",
    "X = tfidf.fit_transform(texts['clean']); y = texts['label']\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=13, stratify=y)\n",
    "\n",
    "clf = LogisticRegression(max_iter=500).fit(Xtr, ytr)\n",
    "pred = clf.predict(Xte)\n",
    "print(classification_report(yte, pred))\n",
    "\n",
    "coefs = clf.coef_[0]\n",
    "fn = tfidf.get_feature_names_out()\n",
    "top_pos = [fn[i] for i in coefs.argsort()[-10:][::-1]]\n",
    "top_neg = [fn[i] for i in coefs.argsort()[:10]]\n",
    "print('Top positive indicators:', top_pos)\n",
    "print('Top negative indicators:', top_neg)\n",
    "\n",
    "cv = CountVectorizer(min_df=1, stop_words='english')\n",
    "Xc = cv.fit_transform(texts['clean'])\n",
    "lda = LatentDirichletAllocation(n_components=3, random_state=0).fit(Xc)\n",
    "terms = cv.get_feature_names_out()\n",
    "for k, comp in enumerate(lda.components_):\n",
    "    top = comp.argsort()[-8:][::-1]\n",
    "    print(f'Topic {k+1}:', [terms[i] for i in top])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
